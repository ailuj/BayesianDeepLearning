{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the introduction it was mentioned that we are going to use Sharpe Ratio adjusted for skewness and kurtosis as a measure of loan application \"goodness\".\n",
    "\n",
    "The Sharpe Ratio (Sharpe (1966)) is defined as:\n",
    "\n",
    "$SR = \\frac{r - i}{SD}$\n",
    "\n",
    "where r stands for expected return (in our case mean of the distribution), i for risk free interest rate (we assumed it is 0), and SD for standard devition, as a measure of a risk.\n",
    "\n",
    "However, as it was explained in the introduton, normal distribution of profit rates seems as unnecessary strong assumption. Hence, Sharpe Ratio, accounting just for mean and variance, is unsuitable.\n",
    "\n",
    "In order to solve this we employ adjustment introduced by Pezier and White (2006), where in addition to mean and standard deviation, we take into account skewness and kurtosis of distribution. Additionally, it should be mentioned that the following formula assumes exponential utility function and further risk-seeking behaviour of economic agents. This assumption seem quite reasonable in our case. The Adjusted for Skewness and Kurtosis Sharpe Ratio (ASKSR):\n",
    "\n",
    "$ASKSR = SR[1 + \\frac{S}{6}SR - \\frac{K - 3}{24} SR^{2}]$ \n",
    "\n",
    "Where K stands for kurtosis and S for skewness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now let's see how our models perform. We calculated ASKSR for each applicant in our test dataset of 33000 observations. Then, on the first half of the test dataset, we found an optimal cutoff point for ASKSR. We did it by summing the profit that is above some assumed ASKSR level. Then, we evaluate the optimal cutoff on the second part of the test sample. We are expressing the result as a percantage of the maximum possible profit (i.e. sum of possitive profits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy  as np\n",
    "from scipy.stats import kurtosis, skew, mode\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimalCut (x,y):\n",
    "        max_y = max(y)\n",
    "        max_x = x[y.index(max_y)]\n",
    "        return (max_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "profit = np.array(pickle.load(open( \"data/y_test_abs.pickle\", \"rb\" )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = []\n",
    "stacked_res = []\n",
    "#MCDrop Results\n",
    "\n",
    "#for filepath in glob.iglob('C:/master/iss/results/Batuhan/mcdrop//*.pickle'):\n",
    "            \n",
    "results = np.array(pickle.load(open( \"results/bayesian/mc_dropout.pickle\", \"rb\" )))\n",
    "\n",
    "df = pd.DataFrame(\n",
    "     {'mean': np.mean(results, axis=0)[:,0],\n",
    "      'variance': np.var(results, axis=0)[:,0],\n",
    "      'skewness': skew(results, axis = 0)[:,0],\n",
    "      'kurtosis': kurtosis(results, axis = 0)[:,0],\n",
    "      'Returns': profit\n",
    "     })\n",
    "\n",
    "df['standardDeviation'] = np.sqrt(df['variance'])\n",
    "df['SR_mean'] = df['mean']/df['standardDeviation']\n",
    "df['ASKSR_mean'] = df['SR_mean'] * (1 + df['skewness'] * df['SR_mean']/6 - (df['kurtosis'] - 3)/24 * (df['SR_mean'] ** 2))\n",
    "\n",
    "    \n",
    "df, df_test = train_test_split(df, test_size=0.5, random_state=42)\n",
    "\n",
    "ASKSR_cutoff_mean = np.linspace(min(df['ASKSR_mean']), max(df['ASKSR_mean']), 10000)\n",
    "\n",
    "ASKSR_profit_mean = []\n",
    "\n",
    "for i in range(0, len(ASKSR_cutoff_mean) - 1, 1):\n",
    "    ASKSR_profit_mean.append(df.loc[df['ASKSR_mean'] > ASKSR_cutoff_mean[i], 'Returns'].sum())   \n",
    "        \n",
    "    \n",
    "ASKSR_mean_Optimal = optimalCut(x = ASKSR_cutoff_mean, y = ASKSR_profit_mean)\n",
    "\n",
    "ASKSR_profit_optimal_mean = []\n",
    "ASKSR_profit_optimal_mean.append(df_test.loc[df_test['ASKSR_mean'] > ASKSR_mean_Optimal, 'Returns'].sum())    \n",
    "    \n",
    "method.append(\"MC Droput\")\n",
    "stacked_res.append(ASKSR_profit_optimal_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VI Results\n",
    "\n",
    "#for filepath in glob.iglob('C:/master/iss/results/Batuhan/mcdrop//*.pickle'):\n",
    "            \n",
    "    results = np.array(pickle.load(open( \"results/bayesian/VariationalInference.pickle\", \"rb\" )))\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {'mean': np.mean(results, axis=0),\n",
    "         'variance': np.var(results, axis=0),\n",
    "         'skewness': skew(results, axis = 0),\n",
    "         'kurtosis': kurtosis(results, axis = 0),\n",
    "         'Returns': profit\n",
    "        })\n",
    "\n",
    "    df['standardDeviation'] = np.sqrt(df['variance'])\n",
    "    df['SR_mean'] = df['mean']/df['standardDeviation']\n",
    "    df['ASKSR_mean'] = df['SR_mean'] * (1 + df['skewness'] * df['SR_mean']/6 - (df['kurtosis'] - 3)/24 * (df['SR_mean'] ** 2))\n",
    "\n",
    "    \n",
    "    df, df_test = train_test_split(df, test_size=0.5, random_state=42)\n",
    "\n",
    "    ASKSR_cutoff_mean = np.linspace(min(df['ASKSR_mean']), max(df['ASKSR_mean']), 10000)\n",
    "\n",
    "    ASKSR_profit_mean = []\n",
    "\n",
    "    for i in range(0, len(ASKSR_cutoff_mean) - 1, 1):\n",
    "        ASKSR_profit_mean.append(df.loc[df['ASKSR_mean'] > ASKSR_cutoff_mean[i], 'Returns'].sum())   \n",
    "        \n",
    "    \n",
    "    ASKSR_mean_Optimal = optimalCut(x = ASKSR_cutoff_mean, y = ASKSR_profit_mean)\n",
    "\n",
    "    ASKSR_profit_optimal_mean = []\n",
    "    ASKSR_profit_optimal_mean.append(df_test.loc[df_test['ASKSR_mean'] > ASKSR_mean_Optimal, 'Returns'].sum())    \n",
    "    \n",
    "    method.append(\"Variational Inference\")\n",
    "    stacked_res.append(ASKSR_profit_optimal_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.array(pickle.load(open( \"results/deterministic/Regularized.pickle\", \"rb\" )))\n",
    "\n",
    "df = pd.DataFrame(\n",
    "        {'mean': results[:,0],\n",
    "         'Returns': profit\n",
    "        })\n",
    "\n",
    "df, df_test = train_test_split(df, test_size=0.5, random_state=42)\n",
    "\n",
    "deterministic_cutoff_mean = np.linspace(min(df['mean']), max(df['mean']), 10000)\n",
    "\n",
    "deterministic_profit_mean = []\n",
    "\n",
    "for i in range(0, len(deterministic_cutoff_mean) - 1, 1):\n",
    "    deterministic_profit_mean.append(df.loc[df['mean'] > deterministic_cutoff_mean[i], 'Returns'].sum())   \n",
    "\n",
    "deterministic_mean_Optimal = optimalCut(x = deterministic_cutoff_mean, y = deterministic_profit_mean)\n",
    "\n",
    "deterministic_profit_optimal_mean = []\n",
    "deterministic_profit_optimal_mean.append(df_test.loc[df_test['mean'] > deterministic_mean_Optimal, 'Returns'].sum())    \n",
    "    \n",
    "method.append(\"Deterministic\")\n",
    "stacked_res.append(deterministic_profit_optimal_mean)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "method.append(\"Naive\")\n",
    "stacked_res.append(sum(df_test['Returns']))\n",
    "\n",
    "method.append(\"Max\")\n",
    "stacked_res.append(df_test.loc[df_test['Returns'] > 0, 'Returns'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_Compare = pd.DataFrame(\n",
    "        {'method': method,\n",
    "         'result': stacked_res\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range (0,3):\n",
    "    results_Compare['result'][j] = results_Compare['result'][j][0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in range (0,4):\n",
    "    results_Compare['result'][m] = results_Compare['result'][m]/results_Compare['result'][4]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are presented in the next table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MC Droput</td>\n",
       "      <td>0.234959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Variational Inference</td>\n",
       "      <td>0.224832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Deterministic</td>\n",
       "      <td>0.231184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naive</td>\n",
       "      <td>0.037151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Max</td>\n",
       "      <td>2.55511e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  method       result\n",
       "0              MC Droput     0.234959\n",
       "1  Variational Inference     0.224832\n",
       "2          Deterministic     0.231184\n",
       "3                  Naive     0.037151\n",
       "4                    Max  2.55511e+07"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_Compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, MC Dropout beats both Deterministic approach and VI by a margin, while all three are way better than naive approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this study, we tried to show that uncertainty is not only something that places obstacle in front of good predictions. We can indeed benefit from it, if we are able to estimate it. It is a difficult task though. Approximate Bayesian variational inference techniques provide us with scalable estimations. Among them, MC dropout yields results those are very close to usual neural network models in performance. MC dropout is easier to use, faster and more scalable. We apply it very intuitively apply it on both training and test times and automatically obtain an approximate probabilistic model.But is just one way of approximating the posterior. Applications of Bayesian inference in deep learning is a reviving field, and now in development. In near future, we are likely to witness better theoretical explorations and algorithmic developments. Not to say further successful applications in finance and many other fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Knight, F. (1921). From Risk, Uncertainty, and Profit. The Economic Nature of the Firm.\n",
    "\n",
    "* Gal., Y.  (2015, July 3). What My Deep Model Doesn’t Know [Web log post]. Retrieved February 8, 2017, from        http://mlg.eng.cam.ac.uk/yarin/blog_3d801aa532c1ce.html\n",
    "\n",
    "* Bernardo, J. M., & Smith, A. F. (1994). Bayesian Theory. Wiley Series in Probability and Statistics.\n",
    "\n",
    "* Neal, R. M. (1996). Bayesian Learning for Neural Networks. Lecture Notes in Statistics.\n",
    "\n",
    "* Betancourt, M., & Girolami, M. (2015). Hamiltonian Monte Carlo for Hierarchical Models. Current Trends in Bayesian  Methodology with Applications. doi:10.1201/b18502-5\n",
    "\n",
    "* Robert, C. P., & Casella, G. (2004). Monte Carlo Optimization. Springer Texts in Statistics Monte Carlo Statistical Methods.\n",
    "\n",
    "* Murphy, K. P. (2012). Machine learning: A probabilistic perspective. Cambridge, England: The MIT Press.\n",
    "\n",
    "* Graves, A. (2011). Practical variational inference for neural networks. Advances in Neural Information Processing Systems.\n",
    "\n",
    "* Ranganath, R., Gerrish, S., & Blei, D. M. (2014). Black Box Variational Inference. Proceedings of the 17th International Conference on Artificial Intelligence and Statistics (AISTATS), 33.\n",
    "\n",
    "* Blei, D. M., Kucukelbir, A., & Mcauliffe, J. D. (2017). Variational Inference: A Review for Statisticians. Journal of the American Statistical Association, 112(518), 859-877.\n",
    "\n",
    "* Gal, Y., & Ghahramani, Z. (2016). Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning. Proceedings of the 33 Rd International Conference on Machine Learning, 43.\n",
    "\n",
    "* Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., &  Salakhutdinov, R. (2014). Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15 (1).\n",
    "\n",
    "* MacKay, D. J. (2004). Information theory, inference, and learning algorithms. Cambridge, U.K.: Cambridge University Press.\n",
    "\n",
    "* Osband, I. (2016). Risk versus Uncertainty in Deep Learning : Bayes , Bootstrap and the Dangers of Dropout.\n",
    "\n",
    "* Sharpe, W. F. (1966). \"Mutual Fund Performance\". Journal of Business. 39 (S1): 119–138. doi:10.1086/294846.\n",
    "\n",
    "* Pezier J, & White A, (2006) \"The relative Merits of Investable Hedge Fund indices and of Funds of Hedge Funds in Optimal Passive Portfolios\"\n",
    "\n",
    "* Andreeva, G., Ansell, J., & Crook, J. (2007). Modelling profitability using survival combination scores. European Journal of Operational Research, 183, 1537-1549.\n",
    "\n",
    "* R T Stewart (2011) A profit-based scoring system in consumer credit: making acquisition decisions for credit cards, Journal of the Operational Research Society, 62:9, 1719-1725, DOI: 10.1057/jors.2010.135 \n",
    "\n",
    "* S M Finlay (2008) Towards profitability: a utility approach to the credit scoring problem, Journal of the Operational Research Society, 59:7, 921-931, DOI: 10.1057/ palgrave.jors.2602394\n",
    "\n",
    "* Luis Javier Sánchez Barrios, Galina Andreeva & Jake Ansell (2014) Monetary and relative scorecards to assess profits in consumer revolving credit, Journal of the Operational Research Society, 65:3, 443-453, DOI: 10.1057/jors.2013.66 \n",
    "\n",
    "* Carlos Serrano-Cinca-Begoña Gutiérrez-Nieto (2016),  The use of profit scoring as an alternative to credit scoring systems in peer-to-peer (P2P) lending, Decision Support Systems, https://doi.org/10.1016/j.dss.2016.06.014\n",
    "\n",
    "* Thomas Verbraken, Cristian Bravo, Richard Weber, Bart Baesens (2014), Development and application of consumer credit scoring models using profit-based classification measures, European Journal of Operational Research\n",
    "\n",
    "* Stefan Lessmann, Bart Baesens, Hsin-Vonn Seow, Lyn C. Thomas (2015), Benchmarking state-of-the-art classification algorithms for credit scoring: An update of research, European Journal of Operational Research\n",
    "\n",
    "* Mee Chi So, Lyn C. Thomas, Hsin-Vonn Seow, Christophe Mues (2014), Using a transactor/revolver scorecard to make credit and pricing decisions, Decision Support Systems\n",
    "\n",
    "* Kaveh Bastani, Elham Asgari, Hamed Namavari (2018), Wide and Deep Learning for Peer-to-Peer Lending, arXiv\n",
    "\n",
    "* Selçuk Bayracı (2017), Application of profit-based credit scoring models using R, Romanian Statistical Review 4/2017\n",
    "\n",
    "* Devavrat Shah and Kang Zhang (2014), Bayesian regression and Bitcoin, 52nd Annual Allerton Conference on Communication, Control, and Computing (Allerton)\n",
    "\n",
    "* Catarina Moreira and Emmanuel Haven and Sandro Sozzo and Andreas Wichert (2017), The Dutch's Real World Financial Institute: Introducing Quantum-Like Bayesian Networks as an Alternative Model to deal with Uncertainty, arXiv\n",
    "\n",
    "* Michael Maio Pires and Tshilidzi Marwala (2007), Option Pricing Using Bayesian Neural Networks, arXiv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
