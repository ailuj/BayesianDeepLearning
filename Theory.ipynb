{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j6xJXKqh1QGY",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining what uncertainty is a philosophical question. Frank Knight distinguished between two types of uncertainty a long time ago. The first type he named as 'empirical uncertainty' being a quantity susceptible of measurement. The second type is the genuine or 'aleatoric uncertainty' which is far out of the reach of the measurement and something that always exists (Knight 1921). Whenever we build a predictive model, our predictions throughout the model reflect both empirical uncertainty caused by noisy observations of data, and aleatoric uncertainty caused by structural relationships within data. By obtaining more data, we can reduce empirical uncertainty. Although we cannot reduce aleatoric uncertainty, we can take its effects under control through model selection or estimating the total uncertainty embodied in our model (model uncertainty)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FTitdBSH1ZfA",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A usual neural network operates by pointwise optimisation of its network weights through maximum likelihood estimation. Ideally, we might want to have a distribution over weights, instead of having only 'the best' weights maximizing a likelihood function. Given that weights establish connections across input data to make predictions, having stochastic weights would result in obtaining a set of predictions. Thereby, we would have a better grasp of model uncertainty that we know relatively 'how certain' we are of a particular prediction. However, deep learning models usually return only point estimates, therefore they are 'falsely overconfident' in their predictions. They basically fail to know 'what they do not know' **(Gal, blog)**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disclaimer: The following section is an extensive literature review, you can skip it for now and return there if there is anything that you do not understand by heart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Literature Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vktGQIsW1Znd",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bayesian inference is a way of quantifying model uncertainty. It is about updating our beliefs on model parameters in the light of new information (Bernardo and Smith 1994). Bayesian inference has a long history in machine learning and it is a resurrecting theme in deep learning research. In Bayesian statistics, the posterior function summarizes all we know about the model parameters given data. There are various ways to estimate the posterior. This estimation cannot be done exactly, but only approximately in most situations. Popular posterior approximations include Markov Chain Monte Carlo (MCMC) sampling and variational inference (VI) approaches. Neural networks are so complex that it is difficult to even approximate the posterior efficiently. Many algorithmic innovations and theoretical explorations were needed for practical applications of Bayesian inference to neural networks. \n",
    "\n",
    "Radford Neal is a pioneer in Bayesian Deep Learning who showed similarities between neural networks and Bayesian Gaussian processes (1993). Hence, it became possible to migrate already-established techniques of Bayesian inference to neural networks. Neal developed Hamiltonian Monte Carlo algorithm (HMC) for the posterior estimation in a multi-dimensional setting by using differential geometry and physics. While MCMC sampling builds a Markov chain whose stationary distribution is the posterior, HMC constructs Markov transitions by lifting into, exploring, and projecting from the expanded state space of the posterior and converges to its stationary distribution much faster than MCMC (Betancourt 2018). MCMC methods guarantee asymptotically exact samples from the posterior (Robert and Casella 2004). They are the closest to an ideal Bayesian inference method. However, they are computationally expensive to apply to large datasets and neural networks. It is also difficult to assess when they converge to the posterior (Murphy 2012). \n",
    "\n",
    "VI provides with a faster but less guaranteed approximate inference than MCMC and HMC. The main idea behind VI is to pick a distribution over a family of distributions which is as close as possible to the posterior. VI frames the posterior approximation as an optimisation problem over function spaces. Therefore, already existing optimisation algorithms in the neural networks literature can be transferred to the field of Bayesian inference. Unlike sampling methods, VI can be extended in complex neural network architectures like LSTM by the virtue of gradient-descent-based techniques (Graves, 2011). It is also worth mentioning the black box variational inference, an algorithm which applies Monte Carlo sampling to the results of stochastic optimisation in order to reduce their noise (Ranganath, Gerrish and Blei 2013). We can scale and speed up the VI even further by using distributed computation. The disadvantage of using VI is that we obtain only the closest function to the posterior we can, and not necessarily the posterior itself. VI does not give any guarantee that we obtain the posterior following any procedure. Whenever we choose VI over MCMC to approximate the posterior, we trade off speed against exactness (Blei et. al. 2018).\n",
    "\n",
    "In this blog post we are particularly interested in a specific way of applying VI without explicitly using any of the above-mentioned techniques. Yarin Gal (2016) argues that it is possible to recast the available deep learning tools as Bayesian models without changing either the models or the optimisation. Stochastic regularization techniques like dropout regularization can be tied to approximate inference in Bayesian models. Dropout is a technique that prevents overfitting by randomly 'dropping out' units of a neural network with a chosen probability. An additional effect of this technique is that we combine $2^n$ different neural network architectures by optimizing only $n$ units. Each time we drop out units, what we obtain is basically a different neural network. Training a neural network with dropout is, therefore, collecting many 'thinned' neural networks. Srivastava et. al. (2014) suggest that we should only use the dropout technique during model training, but not during test time. By this way, our collection of neural networks are averaged properly. It is both an intuitively and computationally easy method of regularisation. But it is not exactly how we obtain Bayesian inference from this regularisation technique.\n",
    "\n",
    "Yarin Gal's (2016) argument is that we should use the dropout technique during **test time** as well. We build a complete probabilistic model by randomly dropping out units before each layer (input, hidden, and output) of a neural network both during the train and test time. When we run this model T times, we would obtain T samples from an approximate posterior distribution for each prediction point. We can analyze the statistical properties of the output, and derive alternative risk measures. That means, according to Gal, we would not need anything else than a dropout regularised network to apply Bayesian inference. Gal and Ghahramani (2016) prove that the optimisation done by the dropout regularisation is mathematically equivalent to one type of VI followed by a Monte Carlo integration to get rid of noisy estimates arising from the optimisation procedure. Therefore, this technique is called MC dropout.\n",
    "\n",
    "Since MC dropout is a type of VI, it still gives an approximation to the posterior without any guarantee of convergence. But it is obvious that MC dropout collects approximate inference even faster and scalable than VI. Furthermore, MC dropout performs usually better in predictions than neural networks trained either by VI or MCMC (Gal and Ghahramani, 2016). However, we should keep in mind that Bayesian inference is not all about making better predictions. It seeks rather an understanding of the latent process that is supposed to have generated our observations. MC dropout does not become more Bayesian than other methods if it performs better than other Bayesian methods. But it gives a predictive boost to a type of approximate Bayesian inference. We can indeed make use of this boost in the credit default model we present here. Nevertheless, in a reinforcement learning setting we might be more careful about using MC Dropout. Ian Osband from Google Deepmind raises a warning that MC dropout conflates approximating uncertainty with risk when it is used with fixed dropout rate (Oswald 2016). It can get dangerous if we insist on using MC dropout in robotics. But we will be safe analyzing lender's profits, as it is theme of the blog post. In our context, MC dropout gives us an opportunity to exploit the uncertainty of the model.\n",
    "\n",
    "In the next section, we will discuss theoretical concepts underlying our study, such as Bayesian inference, VI and MC dropout.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian models are beautiful. They have their own version of Occram's Razor which tells us they do not overfit as easy as usual neural networks with deterministic weights (MacKay 2003). They are also robust to outliers (Ghahramani 2011). It makes particularly sense to use Bayesian inference in situations where it is very expensive to obtain large amount of data, such as DNA-sequencing, or where we need to apply interpolations quite frequently such as in geostatistics or astrophysics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wKinNkvs1Zu-",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Under the perspective of the Bayesian inference, each observation is an opportunity to criticize/update our beliefs about a given a (deep learning) model. Using the Bayes' rule below, we can figure out how the degree of belief in a model ( **the posterior function $P(\\omega|X)$**) is related to the likelihood of data (** the likelihood function $P(X|\\omega)$**) , our knowledge about the data ( **the prior $P(\\omega)$**) and the evidence (**the marginal likelihood $P(X)$**):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o1vqAdJB1ZyJ"
   },
   "source": [
    "$$p(\\omega | X) = \\frac{p(X, \\omega)}{P(X)} \\implies p(\\omega | X) = \\frac{p(X|\\omega)p(\\omega)}{p(X)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ulo3TuVH1Z1R"
   },
   "source": [
    "Having defined the posterior as above, the prediction on new observations *$x_{new}$* is made through model update/criticism on the **posterior predictive distribution**. Note that we $p(x_{new}|X)$ parametrized with $\\omega$ which represents the parameters of any models we would like to use:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lZvNjybd1Zsw"
   },
   "source": [
    "$$p(x_{new}|X) = \\int p(x_{new}| \\omega)p(\\omega | X) d\\omega$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hR4-dPOj1Zql"
   },
   "source": [
    "So far, everything seems fine:  As oppose to deterministic neural networks which maximize **the likelihood function** $P(X|\\omega)$ and obtain only point estimates, we can calculate the **posterior predictive distribution** $P(x_{new}|X) $ and get a distribution of estimations for each observation. Except that it is very difficult in nonlinear models to calculate the posterior directly (Murphy 2012). The reason is that the integral above is intractable because the posterior $P(\\omega|X)$ is intractable. And posterior is intractable because the evidence $P(X)$ is intractable. To see this argument more clearly, let's rewrite the evidence $P(X)$ by using the law of total probability as parametrised by our model's parameters $\\omega$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nmI-HvCzTS5T"
   },
   "source": [
    "$$p(X) = \\int p(X,\\omega)d\\omega$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KLonMR2tTTJT"
   },
   "source": [
    "As we see, the evidence, i.e. the denominator of the posterior function, represents a high-dimensional integral that lacks a general analytic solution. Therefore, calculating the posterior usually means **approximating** it. There is a whole literature out there on how to approximate the posterior as we also discussed in the previous section. After we are done with approximating the posterior, our neural network in a regression problem will look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "colab_type": "code",
    "id": "Fc6uxV0lvCXi",
    "outputId": "f825509a-a7f0-4019-b71f-91955267b978",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=https://i.imgur.com/WY3Elcf.png width=800>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "HTML('<img src=https://i.imgur.com/WY3Elcf.png width=800>')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-NK1EVclTTGr"
   },
   "source": [
    "Observe that for each point in the graph, we have a number of fits. The variance of fitted functions are the highest at the region where we don't have data and it is the lowest where we have a concentration of data. We can also interpret it during prediction in a way that we have the\n",
    "highest aleatoric uncertainty at the region where we have the least clue about our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dCGYBAEQTTEO"
   },
   "source": [
    "\n",
    "We explained what the posterior is. But what about the prior $p(\\omega)$, our domain knowledge of the data? Does our choice of the prior matter? It matters. If we use conjugate priors, we will actually have closed-form solutions for estimating the posterior. Conjugate priors simplify the computation. And in that case it is suggested to choose an uninformative prior which does not depend on data (Murphy 2012). Your choice of the distributional family of the prior can affect the predictions you make. However, in the neural network setting, we usually do not have models with nice conjugate priors. Therefore, it increases complexity of applying variational inference. Successful applications require fine-tuning of the distributional parameters of the prior.In this blog post, we only consider Gaussian priors with lengthscale (function frequency), which is a suitable choice for regression problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jqlkxxvk9Vm3"
   },
   "source": [
    "# Variational Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mL-6LGOvE2mA"
   },
   "source": [
    "The posterior is intractable. We need to approximate it. The most popular two approaches in approximating the posterior are sampling-based methods like Markov chain Monte Carlo (MCMC) and variational inference (VI). Sampling-based methods usually take so much computational resources that they are almost impractical to use in deep learning. We need a shortcut. VI is such a shortcut. The idea is to find the closest function $q(\\omega)$ to the posterior $p(\\omega | x)$. In order to find it, we minimize the Kullback-Leibler divergence (KL divergence) of $q(\\omega)$ from $p(\\omega | x)$. In other words, we minimize $KL( q(\\omega)  ||  p(\\omega | x) )$. To do so, we assume that $\\omega$ is parametrized by some latent variable $\\theta$ (here, the latent of the latent, which we also call 'variational parameters'), and apply minimization with respect to $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A_3Bt_47E2ok"
   },
   "source": [
    "$$\\underset{\\theta}{\\operatorname{min}}KL(q(\\omega ; \\theta) || p(\\omega | x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rxg9GfvGE2rM"
   },
   "source": [
    "Note that KL divergence is non-symmetric, meaning that reversing the arguments will lead us to a totally different method. Also note that what we are doing here is optimizing functions to find the best functional  representation of the posterior $p(\\omega|x)$. This optimization belongs to the field of 'calculus of variations' (Bishop 2006). To think about this optimization problem, we can rewrite below what KL divergence actually is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U63KlNtGE2t4"
   },
   "source": [
    "$$\\underset{\\theta}{\\operatorname{min}}KL(q(\\omega ; \\theta) || p(\\omega | x)) \n",
    "\\iff  \n",
    "\\underset{\\theta}{\\operatorname{min}} \\mathop{\\mathbb{E}}_{q(\\omega | \\theta)}[ logq(\\omega; \\theta) - log p(\\omega | x)]$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M1k-FXz8E2wY"
   },
   "source": [
    "One question immediately appears: How do we minimize the distance to the posterior if we don't even know what the posterior is? It is a nontrivial question. We can have a clue about the posterior only if we have data. Let's play with the KL divergence if we can recover the evidence $p(x)$ any where around. We will rewrite the posterior by using the Bayes's rule:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5dimZfdRE2zL"
   },
   "source": [
    "$$\\mathop{\\mathbb{E}}[ logq(\\omega; \\theta) - log p(\\omega | x)] = \\mathop{\\mathbb{E}}[ logq(\\omega; \\theta) - log \\frac{p(x,\\omega)}{p(x)}]$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qKxuT3rcE21d"
   },
   "source": [
    "Reorganizing the expectation above gives us the evidence lower bound $ELBO(\\theta)$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "88_WdpVlhmXG"
   },
   "source": [
    "$$KL(q(\\omega ; \\theta)||p(\\omega | x))= -\\mathop{\\mathbb{E}}[log p(x,\\omega)-logq(\\omega;\\theta)]+\\mathop{\\mathbb{E}}logp(x)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w4x-4rWGYoB9"
   },
   "source": [
    "$$ELBO(\\theta)=\\mathop{\\mathbb{E}}[logp(x,\\omega)-logq(\\omega;\\theta)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gLbLQ0LbiZ9w"
   },
   "source": [
    "As we see, $logp(x)$ does not depend on $\\theta$. That means minimizing $KL(q(\\omega ; \\theta) || p(\\omega | x))$ is the same thing as maximizing ELBO($\\theta$) which we call the evidence lower bound. That means, it is something that makes sense to maximize. We can prove that ELBO($\\theta$) is a lower bound to the evidence P(x) easily by using Jensen's Inequality. If you are interested, you can check out the full derivations in the documentation page of the python library Edward, http://edwardlib.org/tutorials/klqp.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=https://i.imgur.com/UZBvbq1.png>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "HTML('<img src=https://i.imgur.com/UZBvbq1.png>')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "roDy7OUohRYT"
   },
   "source": [
    "We have a graphical explanation above. We maximize the evidence lower bound as to attain the closest function to the posterior as we can. There are still some KL divergence of $q(\\omega ; \\theta)$ from the real posterior $p(\\omega | x)$. We are not guaranteed to reduce this difference completely. It is the drawback of implementing this approach. We cannot make this KL-divergence zero in general and might not really attain the real posterior, regardless of how ambitious we are in optimisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KGBCTKZbiJPS"
   },
   "source": [
    "As you'll wonder, the ELBO($\\theta$) is a non-convex optimisation objective and there are many ways to minimize ELBO($\\theta$). We can apply variants of stochastic gradient descent. In this paper, we are going to use ADVI algorithm from the PyMC3 package. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n6l9P3DhYV7v"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r434Vnu7bNa2"
   },
   "source": [
    "# Monte Carlo Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3tBwXlcNbNep"
   },
   "source": [
    "Scared by all those mathematical derivations of the variational inference? Good news to you: An interesting finding in deep learning research suggests that you can apply variational inference without even knowing all those derivations. And it is going to be much faster and applicable. Yarin Gal (2016) suggests that we are doing something very close to a type of variational inference each time we regularize our deterministic neural network with dropout technique. According to his PhD thesis, all we need is to apply dropout during both training and test time, as opposed to the usual application of dropout only during model training (Gal, 2016). Let's review what the usual dropout is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab_type": "text",
    "id": "C3DFlFuqldkE"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=https://cdn-images-1.medium.com/max/1200/1*iWQzxhVlvadk6VAJjsgXgg.png width=600>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML \n",
    "HTML('<img src=https://cdn-images-1.medium.com/max/1200/1*iWQzxhVlvadk6VAJjsgXgg.png width=600>')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fqLwRDoYG10u"
   },
   "source": [
    "If we build a complicated neural network with lots of hidden layers and cells on a limited amount of training data, our model would probably memorize the data and does not generalize well. This phenomenon is called **overfitting**. Dropout is a regularizer to avoid overfitting by disabling some cells in hidden layers with some probability (Srivastava et. al. 2014). By doing this, dropout effectively samples from an exponential number of different networks in a tractable and feasible way. Dropout is computationally cheap and it can be applied in nearly any neural network architecture with ease. Wenn applying dropout, we basically do not change anything in our neural network in terms of optimisation and model architecture. We only add a specific l2 weight regularisation term (which corresponds to choosing a prior) and dropout regularisation (which makes sampling from the posterior automatically) before each layer (input, hidden, output). Additionally, we need to apply dropout both in the training and test periods as opposed to its usual implementation. That's all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ny93evqGldyF"
   },
   "source": [
    "Yarin Gal tied up several derivations of stochastic variational inference to those of the stochastic regularization techniques, including dropout. Encouraged by this finding, he suggests to use dropout during test time in order to obtain approximate samples from the posterior function $p(\\omega | x)$. When we apply dropout during test time, we obtain different results each time we run the model. They are approximate samples from the posterior predictive distribution. Gal calculates unbiased estimators for the mean and the variance of the posterior predictive distribution as the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider $y=f^{\\hat{\\omega}}(x)$ as the output of the Bayesian NN, and t= 1,.., T are samples from the posterior predictive distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qjbAFABoldpJ"
   },
   "source": [
    "$$\\mathop{\\mathbb{\\hat{E}}}(y)=\\frac{1}{T}\\sum_{t=1}^{T}f^{\\hat{\\omega_t}}(x)$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathop{\\mathbb{\\hat{E}}}(y^{T}y) = \\tau^{-1}I + \\frac{1}{T}\\sum_{t=1}^{T}f^{\\hat{\\omega_t}}(x)^{T}f^{\\hat{\\omega_t}}(x) -  \\mathop{\\mathbb{\\hat{E}}}(y)^{T} \\mathop{\\mathbb{\\hat{E}}}(y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kPkGc8y1ld1O"
   },
   "source": [
    "Note that $f^{\\hat{\\omega}}(x)$ is a row vector. The mean of our posterior predictive samples is an unbiased estimator of the mean of the approximate distribution $q(\\omega)$. The sample variance plus a term $\\tau^{-1}I$ is also an unbiased estimator of the variance of $q(\\omega)$. That means, with only a small adjustment made to the sample variance, we get the Bayesian results very handy. The adjusting term $\\tau$ equals to the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WS9JlNwxldwc"
   },
   "source": [
    "$$\\tau = \\frac{(1-p)l^{2}}{2N\\lambda}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NbXRkEs6ldvQ"
   },
   "source": [
    "N is the number of data points. $l$ is the prior lengthscale capturing our subjective belief over the prior's frequency. A short length-scale $l$ corresponds to high frequency prior, and a long length-scale corresponds to low frequency prior. $\\lambda$ corresponds to the weight decay regularization term which we additionally use to regularize weight optimization. As a trick during application we will play the equation above, and leave the $\\lambda$ alone:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\lambda = \\frac{(1-p)l^{2}}{2N\\tau}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best $p$, $\\tau$ and $\\lambda$ can be found by cross-validation. They are the only paramteres to optimize when using the MC dropout approach. It is a big relief considering the fact that sampling methods like MCMC and HMC usually require many parameters to optimize. After choosing $\\lambda$, $p$ and $\\tau$, all we need is to calculate the l2 weight-decay regularisation term $\\lambda$ and apply this additional regularisation to our neural network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TTT.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
